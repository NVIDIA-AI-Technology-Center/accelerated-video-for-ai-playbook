{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f6840e",
   "metadata": {},
   "source": [
    "# Video Libraries Tests\n",
    "\n",
    "This notebook focuses on testing video loading libraries. There are many tools one can use to run dataloading for video. But we will focus on which functionality is most needed for ML/DL applications.\n",
    "\n",
    "If we think a minute about DL/ML we realize we need to access data randomly, efficiently and fast. Therefore, our goal is to have a **seek** functionality that is as quick and as lightweight as possible to load the frames we need for each batch. In addition to that, we would like to easily build batches. \n",
    "\n",
    "There are a some common tools we'll look at:\n",
    " - cv2\n",
    " - pyAV\n",
    " - decord\n",
    " - DALI\n",
    " - pyNvVideoCodec\n",
    "\n",
    "### Note on time measurements\n",
    "\n",
    "Please note that to measure operations that happen on GPU, natively parallel and asyncronous chip you should use CUDA Events. In this notebook we use PyTorch to interface with CUDA Events. Be aware that you will have to setup start and end events, initialize the recording when your script starts, trigger end recording when done and *very important* synchronize before computing the elapsed time. Another good practice is to run your function/script for a few warmup iterations before timing, to skip measuring all the initialization overhead.\n",
    "\n",
    "### Disclaimer on Implementations\n",
    "\n",
    "The video libraries tests are a best effort and not to be considered a professional benchmark. If you find errors, optimizations, improvements please open an issue and we will improve them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17483119",
   "metadata": {},
   "source": [
    "## Base Implementations\n",
    "\n",
    "Let's give a look at a simple implementation of the seek function before we move on to the **real businness** of building batches. \n",
    "\n",
    "First, for a single video there are five toy examples below, based on opencv, decord-cpu, decord-gpu, pyav, and pyNvVideoCodec, implemented in [toy_cv2.py](assets/toy_cv2.py), [decord_cpu.py](assets/toy_decord.py), [toy_decord_gpu.py](assets/toy_decord_gpu.py), [toy_pyav.py](assets/toy_pyav.py), [toy_pynvvideocodec](assets/toy_pynvvideocodec.py) respectively.\n",
    "If we look at them in detail, we can observe that opencv, pyav, pyNvVideoCodec go through the video in a sequential order with more or less compact python implementations. Instead decord is much more pythonic and treats the video as a list of frames we can randomly access.\n",
    "Moving on to the loading time, decord is on par with opencv but pyav is slower. pyNvVideoCodec instead, uses a GPU accelerated approach, making it faster than pyav but slower than opencv on this test. \n",
    "\n",
    "Second, moving on to the *real* task of building batches, things get more interesting. As before we have five examples, based on opencv, decord-cpu, decord-gpu, pyav, and pyNvVideoCodec, implemented in [toy_batch_cv2.py](assets/toy_batch_cv2.py), [toy_decord_batch_cpu.py](assets/toy_batch_decord.py), [toy_batch_decord_gpu.py](assets/toy_batch_decord_gpu.py), [toy_batch_pyav.py](assets/toy_batch_pyav.py), [toy_batch_pynvvideocodec](assets/toy_batch_pynvvideocodec.py) respectively.\n",
    "Opencv, pyav, pyNvVideoCodec need to loop over the video again and again to find the frames to load, this makes the code less compact and more verbose. Decord instead can directly load frames from the video with a `get_batch` function where we pass the frame indexes.\n",
    "These differences translate in a small processing time advantage for decord. Second fastest is pyNvVideoCodec that is highly optimized to run on GPU using hardware based GPU video decoding, this is a potential huge advantage because it allows to pass data with *zero-copy* (*i.e.* the data is already ready to use in GPU, no host to device copy needed). More in detail for decord we can observe that on a single video the CPU version is faster than the GPU version, we will analyze later on a real use case if these performance change.\n",
    "\n",
    "Please keep in mind the exact times of these runs can vary based on the machine you are running on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a035caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 assets/toy_cv2.py\n",
    "! python3 assets/toy_batch_cv2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b08ee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 assets/toy_decord.py\n",
    "! python3 assets/toy_batch_decord.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8be0ab3-e270-4dd8-a139-98f9bf25d960",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 assets/toy_decord_gpu.py\n",
    "! python3 assets/toy_batch_decord_gpu.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ed4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 assets/toy_pyav.py\n",
    "! python3 assets/toy_batch_pyav.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08264d9f-c334-4e36-80d8-3c09c06c169f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 assets/toy_pynvvideocodec.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9e9c96-e448-4554-8260-2cbbd916e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 assets/toy_batch_pynvvideocodec.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e21eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "libraries = (\"cv2\", \"pyAV\", \"decord CPU\", \"decord GPU\", \"PyNvVideoCodec\")\n",
    "values = {\n",
    "    'Single frame': (0, 0, 0, 0, 0),\n",
    "    'Batched frames': (0, 0, 0, 0, 0),\n",
    "}\n",
    "\n",
    "x = np.arange(len(libraries))  # the label locations\n",
    "width = 0.25  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(layout='constrained')\n",
    "\n",
    "for attribute, values in values.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, values, width, label=attribute)\n",
    "    ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('time (s)')\n",
    "ax.set_title('Time by libraries')\n",
    "ax.set_xticks(x + width, libraries)\n",
    "ax.legend(loc='upper left', ncols=3)\n",
    "ax.set_ylim(0, 2.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a035299",
   "metadata": {},
   "source": [
    "The plot above reports the run times obtained from the toy runs. These runs help us understand the behaviour of different libraries on a single video, loading sequentially frames at fixed intervals (blue line) and loading random batches of random frames (orange line), in these experiments we set batch size 8.\n",
    "\n",
    "The first take away is that without a highly efficient seek function that allows us to read at random locations in the video container it is quite challenging to have great dataloading performance.\n",
    "\n",
    "Secondly, take time to look at the code in the [assets](assets) folder, you will see that some libraries are much more verbose than others.\n",
    "\n",
    "Finally, we need to keep in mind that for a ML training we need data in GPU, and libraries that provide hardware (HW) decoding and zero copy access to the frames in GPU are nice to have to reduce data transfers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3247a0",
   "metadata": {},
   "source": [
    "## Dataloaders\n",
    "\n",
    "Let's now move on to more practical implementations. Assuming in your day-to-day experiments you are using PyTorch we will now continue using some previous tests (not all) and see how they work in more practical setting.\n",
    "\n",
    "We will see the following:\n",
    " - PyTorch native dataloader (based on pyAV)\n",
    " - DALI (NVIDIA accelerated dataloading library)\n",
    " - decord (CPU and GPU implementations)\n",
    " - pyNvVideoCodec\n",
    "\n",
    "The dataset used for this example is [UCF101](https://www.crcv.ucf.edu/data/UCF101.php) which is small but not tiny allowing us to create a meaninigful experiment.\n",
    "\n",
    "### PyTorch\n",
    "\n",
    "PyTorch implements in the torchvision library many useful functions for image and video processing. The developers chose to use PyAV for video loading under the hood.\n",
    "\n",
    "*Note* we learned the hard way that the dataloader breaks if it finds unexpected files in the dataset folder. For example I had a script for data preprocessing in the dataset folder and some preprocessed videos, remember to save them in another place!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f89a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35709e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a bug in this dataloader apparently\n",
    "# https://github.com/pytorch/vision/issues/2265\n",
    "def custom_collate(batch):\n",
    "    filtered_batch = []\n",
    "    for video, _, label in batch:\n",
    "        filtered_batch.append((video, label))\n",
    "    return torch.utils.data.dataloader.default_collate(filtered_batch)\n",
    "\n",
    "UCF101_train = torchvision.datasets.UCF101(root='/workspace/playbooks/video/assets/UCF101',\n",
    "                                     annotation_path='/workspace/playbooks/video/assets/UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist',\n",
    "                                     frames_per_clip=8,\n",
    "                                     train=True)\n",
    "\n",
    "dataloaderUFC101 = torch.utils.data.DataLoader(UCF101_train, batch_size=4, shuffle=True, drop_last=True, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d65fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloaderUFC101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdef9c84-2e91-4c51-85e2-265880349810",
   "metadata": {},
   "source": [
    "**Note on time measurement (part 2)**\n",
    "Below we do a few things to measure the time first we use CUDA Events, to make sure we record the GPU asyncronous execution and not the CPU. Secondly we follow a precise sequence of commands, we set a start and end event, record the start, record the end and syncronize them to make sure the CUDA kernels finished execution. To start we also introduces a very short warmup loop to make sure the GPU is ready to go without measuring initial variables setup or other transients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de19f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "# init\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "# warmup the GPU\n",
    "for i, (video, label) in enumerate(dataloaderUFC101):\n",
    "    video.to(device)\n",
    "    label.to(device)\n",
    "    if i == 9:\n",
    "        break\n",
    "# start\n",
    "start.record()\n",
    "# do your things\n",
    "for i, (video, label) in enumerate(dataloaderUFC101):\n",
    "    video.to(device)\n",
    "    label.to(device)\n",
    "    if i == 499:\n",
    "        break\n",
    "    #print(video.size())\n",
    "# end + sync\n",
    "end.record()\n",
    "torch.cuda.synchronize(device)\n",
    "\n",
    "pytorch_time = start.elapsed_time(end)/1000 # is is in ms\n",
    "pytorch_time_batch = pytorch_time / i\n",
    "print('It took %f seconds for %d batches, or %f second per batch' %(pytorch_time, i, pytorch_time_batch) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7c1a0a-7ecf-4b28-adde-0759b72fc5fa",
   "metadata": {},
   "source": [
    "In the cell above to make the experiment fast for you we stop the execution after 500 batches are loaded and moved to the GPU. Running on the full dataset is slightly faster because the initialization cost is diluted over more iterations.\n",
    "\n",
    "The advantage of the native dataloader is that if your dataset is supported you can get away with one line of code. Just choose the parameters of your interest and pass the dataset path to it. The drawback might be less flexibility, and potentially slower compared to DALI for example.\n",
    "\n",
    "*Note* there is some overhead in creating clips from videos when the Dataset is built, so the training will run only after this step is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7395684",
   "metadata": {},
   "source": [
    "### DALI\n",
    "\n",
    "[DALI](https://docs.nvidia.com/deeplearning/dali/user-guide/docs/index.html) is the NVIDIA accelerated DAta loading LIbrary and implements a lot of useful dataloading pipelines for images, video, audio. \n",
    "\n",
    "DALI does not handle variable frame rate and variable frame size, please preprocess the dataset with [preprocess.sh](assets/preprocessassets/preprocess.sh).\n",
    "\n",
    "If you need to install DALI (comes installed natively in NVIDIA NGC docker images) you will need to use the following pip install command, remember to choose the correct cuda version for your environment.\n",
    "```bash\n",
    "pip install --extra-index-url https://pypi.nvidia.com --upgrade nvidia-dali-cuda120\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ce94abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from nvidia.dali import pipeline_def\n",
    "import nvidia.dali.fn as fn\n",
    "import nvidia.dali.types as types\n",
    "from nvidia.dali.plugin.pytorch import DALIGenericIterator\n",
    "from nvidia.dali.plugin.base_iterator import LastBatchPolicy\n",
    "import nvidia.dali.types as types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b0d816",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "sequence_length = 8\n",
    "stride = 1\n",
    "initial_prefetch_size = 16\n",
    "video_directory = \"/workspace/playbooks/video/assets/output/\"\n",
    "\n",
    "video_files = []\n",
    "for train_file in ['trainlist01', 'trainlist02', 'trainlist03']:\n",
    "    train_file = open(\"/workspace/playbooks/video/assets/UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist/\" + train_file + \".txt\", \"r\")\n",
    "    video_files += [video_directory + \"/\" + f.split()[0].split('.')[0] + \".mp4\" for f in train_file]\n",
    "\n",
    "print(video_files[100])\n",
    "print(len(video_files))\n",
    "n_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7ece6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline_def\n",
    "def video_pipe(filenames):\n",
    "    videos = fn.readers.video(\n",
    "        device=\"gpu\",\n",
    "        name=\"VideoReader\",\n",
    "        filenames=filenames,\n",
    "        sequence_length=sequence_length,\n",
    "        stride=stride,\n",
    "        image_type=types.RGB,\n",
    "        pad_sequences=True,\n",
    "        pad_last_batch=True,\n",
    "        shard_id=0,\n",
    "        num_shards=1,\n",
    "        random_shuffle=True,\n",
    "        initial_fill=initial_prefetch_size,\n",
    "        dtype=types.UINT8,\n",
    "        skip_vfr_check=False,\n",
    "    )\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094f9d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "pipe = video_pipe(\n",
    "    batch_size=batch_size,\n",
    "    num_threads=2,\n",
    "    device_id=0,\n",
    "    filenames=video_files,\n",
    "    seed=123456,\n",
    ")\n",
    "pipe.build()\n",
    "\n",
    "dali_iter = DALIGenericIterator(pipe, [\"data\"],\n",
    "                                reader_name=\"VideoReader\",\n",
    "                                dynamic_shape=True,\n",
    "                                last_batch_policy=LastBatchPolicy.DROP)\n",
    "\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "for i, data in enumerate(dali_iter):\n",
    "    if i == 9:\n",
    "        break\n",
    "    #print(\"%d, %s\" % (i, data[0][\"data\"].size()))\n",
    "    video = data[0][\"data\"]\n",
    "\n",
    "start.record()\n",
    "for i, data in enumerate(dali_iter):\n",
    "    if i == 2000:\n",
    "        break\n",
    "    #print(\"%d, %s\" % (i, data[0][\"data\"].size()))\n",
    "    video = data[0][\"data\"]\n",
    "    #sequences_out = pipe_out[0]#.as_cpu().as_array()\n",
    "    #print(sequences_out.shape)\n",
    "end.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "dali_time = start.elapsed_time(end)/1000 # is is in ms\n",
    "dali_time_batch = dali_time / i\n",
    "print('It took %f seconds for %d batches, or %f second per batch' %(dali_time, i, dali_time_batch) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eacf5a2-4932-47dc-96c0-c5171765cfe2",
   "metadata": {},
   "source": [
    "DALI is very similar to the native PyTorch dataloaders to use. It has similar features and accelerates the video loading directly in GPU. Thanks to GPU acceleration it might be faster than PyTorch so a very valid alternative with similar low coding effort."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37897678",
   "metadata": {},
   "source": [
    "### Decord CPU\n",
    "\n",
    "It is common practice in decord to load a frame buffer from each video separately, decord provides also a `VideoLoader` class. We provide an example of VideoLoader usage before moving on to the more common used dataloader implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8697cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import decord\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2d38a2-8736-4b34-a5c7-71570950d361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decord import VideoLoader, VideoReader\n",
    "from decord import cpu, gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5309070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "sequence_length = 8\n",
    "stride = 1\n",
    "initial_prefetch_size = 16\n",
    "video_directory = \"/workspace/playbooks/video/assets/UCF101\"\n",
    "\n",
    "video_files = []\n",
    "for train_file in ['trainlist01', 'trainlist02', 'trainlist03']:\n",
    "    train_file = open(\"/workspace/playbooks/video/assets/UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist/\" + train_file + \".txt\", \"r\")\n",
    "    video_files += [video_directory + \"/\" + f.split()[0] for f in train_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715cabfc-f333-49fd-a116-24734099a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "vl = VideoLoader([video_files[0]], ctx=[cpu(0)], shape=(batch_size, 320, 240, 3), interval=stride, skip=5, shuffle=1)\n",
    "decord.bridge.set_bridge('torch')\n",
    "device = \"cuda:0\"\n",
    "\n",
    "print('Total batches:', len(vl))\n",
    "tic = time.time()\n",
    "for batch in vl:\n",
    "    frames = batch[0].to(device)\n",
    "    labels = batch[1].to(device)\n",
    "\n",
    "toc = time.time()-tic\n",
    "print('Iterated over the video batches in %f sec, %f sec per iter' %(toc, toc/len(vl)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95620df0",
   "metadata": {},
   "source": [
    "Common practice is to create a loader for each video in the dataset, so let's define our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6da8f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class DecordVideoDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the videos.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.video_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Load video paths and labels from directory structure\n",
    "        for label_idx, action in enumerate(sorted(os.listdir(root_dir))):\n",
    "            action_dir = os.path.join(root_dir, action)\n",
    "            if os.path.isdir(action_dir):\n",
    "                for video_file in os.listdir(action_dir):\n",
    "                    if video_file.endswith('.avi'):  # Assuming videos are in .avi format\n",
    "                        self.video_paths.append(os.path.join(action_dir, video_file))\n",
    "                        self.labels.append(label_idx)\n",
    "                        \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load video using Decord\n",
    "        vr = VideoReader(video_path, ctx=cpu(0))  # Use GPU if available: ctx=decord.gpu(0)\n",
    "        \n",
    "        # Get frames\n",
    "        frames_idx = np.random.randint(0, len(vr), sequence_length)\n",
    "        frames = vr.get_batch(frames_idx) # Returns all frames\n",
    "        \n",
    "        # Apply any transformations (e.g., resizing, normalization)\n",
    "        if self.transform:\n",
    "            frames = self.transform(frames)\n",
    "        \n",
    "        return frames, label    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3343a2cc-b52c-4527-8a70-ce910016e2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ucf101_dataset = DecordVideoDataset(root_dir=video_directory, transform=None)\n",
    "dataloader = DataLoader(ucf101_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84739f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "for i, data in enumerate(dataloader):\n",
    "    video = data[0].to(device)\n",
    "    label = data[1].to(device)\n",
    "    #print(video.size())\n",
    "    #print(label.size())\n",
    "    if i == 9:\n",
    "        break\n",
    "\n",
    "start.record()\n",
    "for i, data in enumerate(dataloader):\n",
    "    if i == 1000:\n",
    "        break\n",
    "    video = data[0].to(device)\n",
    "    label = data[1].to(device)\n",
    "    #print(video.size())\n",
    "    #print(label.size())\n",
    "\n",
    "end.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "decord_time = start.elapsed_time(end)/1000 # is is in ms\n",
    "decord_time_batch = decord_time / i\n",
    "print('It took %f seconds for %d batches, or %f second per batch' %(decord_time, i, decord_time_batch) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4e3c83-5e1e-4306-b895-d4002d87771a",
   "metadata": {},
   "source": [
    "Researchers like to use decord due to its flexibility and ease of personalization of the dataloader if non standard operations need to be performed on the video clips."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f844d5f1-7d23-4ba3-b364-ca6e66833bed",
   "metadata": {},
   "source": [
    "### Decord GPU\n",
    "\n",
    "To use the GPU implementation there is only one very simple change to implement. Pass as context to decord the gpu context.\n",
    "In the `__getitem__` function you can see that `VideoReader` gets `ctx=gpu(0)` instead of `ctx=cpu(0)`.\n",
    "\n",
    "**WARNING** we encountered some silent failures when using decord GPU, so we don't encourage you to use it at the moment. Stay tuned, we might find the bug and fix it later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d207f3f2-2147-40e7-b27f-301016a392f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class DecordVideoDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the videos.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.video_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Load video paths and labels from directory structure\n",
    "        for label_idx, action in enumerate(sorted(os.listdir(root_dir))):\n",
    "            action_dir = os.path.join(root_dir, action)\n",
    "            if os.path.isdir(action_dir):\n",
    "                for video_file in os.listdir(action_dir):\n",
    "                    if video_file.endswith('.avi'):  # Assuming videos are in .avi format\n",
    "                        self.video_paths.append(os.path.join(action_dir, video_file))\n",
    "                        self.labels.append(label_idx)\n",
    "                        \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load video using Decord\n",
    "        vr = VideoReader(video_path, ctx=gpu(0))  # Use GPU if available: ctx=decord.gpu(0)\n",
    "        \n",
    "        # Get frames\n",
    "        frames_idx = np.random.randint(0, len(vr), sequence_length)\n",
    "        frames = vr.get_batch(frames_idx) # Returns all frames\n",
    "        \n",
    "        # Apply any transformations (e.g., resizing, normalization)\n",
    "        if self.transform:\n",
    "            frames = self.transform(frames)\n",
    "        \n",
    "        return frames, label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "08dd22cb-eeba-4c36-880e-33192e99f8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ucf101_dataset = DecordVideoDataset(root_dir=video_directory, transform=None)\n",
    "dataloader = DataLoader(ucf101_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb2b06-5b54-4412-9d9c-3948dbbbe9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "for i, data in enumerate(dataloader):\n",
    "    video = data[0].to(device)\n",
    "    label = data[1].to(device)\n",
    "    #print(video.size())\n",
    "    #print(label.size())\n",
    "    if i == 2:\n",
    "        break\n",
    "\n",
    "start.record()\n",
    "for i, data in enumerate(dataloader):\n",
    "    if i == 5:\n",
    "        break\n",
    "    video = data[0].to(device)\n",
    "    label = data[1].to(device)\n",
    "    #print(video.size())\n",
    "    #print(label.size())\n",
    "\n",
    "end.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "decord_gpu_time = start.elapsed_time(end)/1000 # is is in ms\n",
    "decord_gpu_time_batch = decord_gpu_time / i\n",
    "print('It took %f seconds for %d batches, or %f second per batch' %(decord_gpu_time, i, decord_gpu_time_batch) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d230af-f327-45c7-b96d-d48f54dc1ca2",
   "metadata": {},
   "source": [
    "### PyNvVideoCodec\n",
    "\n",
    "What is pyNvVideoCodec? As explained very well in the [docs](https://docs.nvidia.com/video-technologies/pynvvideocodec/pynvc-api-prog-guide/index.html): PyNvVideoCodec is a library that provides Python bindings over C++ APIs for hardware-accelerated video encoding and decoding. Internally, it utilizes core APIs of NVIDIA Video Codec SDK and provides the ease-of-use inherent to Python. It relies on an external FFmpeg library for demuxing media files. Here is a high level block diagram showing client application, PyNvVideoCodec library and related components.\n",
    "\n",
    "In brief, with this library we exploit the power of hardware accelerated video decoding to accelerate dataloading. It is very easy to install `pip install pycuda pynvvideocodec` and soon will be even easier to use with release 2.0 that implements a built in *seek* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "374f05e1-e9a3-4846-bd84-64f5b94a4bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "batch_size = 4\n",
    "sequence_length = 8\n",
    "stride = 1\n",
    "initial_prefetch_size = 16\n",
    "video_directory = \"/workspace/playbooks/video/assets/UCF101\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6cf11684-6ffd-4f19-a20c-f8f6a1f9c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import PyNvVideoCodec as nvc\n",
    "import pycuda.driver as cuda\n",
    "import torch \n",
    "import numpy as np\n",
    "\n",
    "class PyNvCVideoDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the videos.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.video_paths = []\n",
    "        self.labels = []\n",
    "        self.enable_async_allocations = False\n",
    "        \n",
    "        # Load video paths and labels from directory structure\n",
    "        for label_idx, action in enumerate(sorted(os.listdir(root_dir))):\n",
    "            action_dir = os.path.join(root_dir, action)\n",
    "            if os.path.isdir(action_dir):\n",
    "                for video_file in os.listdir(action_dir):\n",
    "                    if video_file.endswith('.mp4'):  # Assuming videos are in .avi format\n",
    "                        self.video_paths.append(os.path.join(action_dir, video_file))\n",
    "                        self.labels.append(label_idx)\n",
    "                        \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def load_batch(self, path_to_video, batch_indeces):\n",
    "        start = cuda.Event()\n",
    "        end = cuda.Event()\n",
    "\n",
    "        start.record(stream=cuda_stream_nv_dec)\n",
    "        demuxer = nvc.CreateDemuxer(path_to_video)\n",
    "        decoder = nvc.CreateDecoder(\n",
    "            gpuid=0,\n",
    "            codec=demuxer.GetNvCodecId(),\n",
    "            cudacontext=cuda_ctx.handle,\n",
    "            cudastream=cuda_stream_nv_dec.handle,\n",
    "            usedevicememory=True,\n",
    "            enableasyncallocations=self.enable_async_allocations,\n",
    "        )\n",
    "    \n",
    "        frame_count = 0\n",
    "        frames = None\n",
    "        for packet_index, packet in enumerate(demuxer):\n",
    "            for frame in decoder.Decode(packet):\n",
    "                frame_count += 1\n",
    "                if frame_count in batch_indeces:\n",
    "                    if frames is None:\n",
    "                        frames = torch.from_dlpack(frame).unsqueeze(0)\n",
    "                    else:\n",
    "                        frames = torch.cat((frames, torch.from_dlpack(frame).unsqueeze(0)), dim=0)\n",
    "\n",
    "        end.record()\n",
    "        end.synchronize()\n",
    "        return frames\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Open a demuxer\n",
    "        demuxer = nvc.CreateDemuxer(video_path) \n",
    "        # check video lenght\n",
    "        for frame_count, packet in enumerate(demuxer):\n",
    "            continue\n",
    "        \n",
    "        # Get frames\n",
    "        frames_idx = np.floor(np.linspace(1, frame_count, sequence_length))\n",
    "        frames = self.load_batch(video_path, frames_idx) # Returns all frames\n",
    "        # Apply any transformations (e.g., resizing, normalization)\n",
    "        if self.transform:\n",
    "            frames = self.transform(frames)\n",
    "        \n",
    "        return frames, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d621337e-f53c-4737-9d05-0e4e3a3683cd",
   "metadata": {},
   "source": [
    "Here the code runs in a script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921eec94-84b6-45bc-b3c1-0ee0e5b2e7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 assets/pynvc_dataloader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c219652-59d3-4aa5-92e6-7be079fb3d44",
   "metadata": {},
   "source": [
    "Thanks to the HW + SW acceleration, CUDA zero copy to PyTorch and flexibility it is a very solid alternative to decord.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f778d3d-3e0e-4ebf-85dc-86a4b804c65a",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "To conclude some libraries allow us to have much better throughput than others, thus maximizing the GPUs utilization. In a multimodal training this is crucial to deliver results in a reasonable time.\n",
    "\n",
    "Going in order we have the pair that requires less coding: DALI and PyTorch dataloader.\n",
    "\n",
    "For maximum customization instead, decord and pyNvVideoCodec are more interesting. However, at the moment we had silent failures with decord-GPU. So (for now) prefer pyNvVideoCodec or decord-CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5791143-e01b-419a-98a7-c2d220951dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
